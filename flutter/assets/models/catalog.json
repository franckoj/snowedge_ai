{
  "models": [
    {
      "id": "qwen3-0.6b-q8",
      "name": "Qwen3 0.6B (Q8)",
      "description": "Compact model with thinking mode for reasoning tasks. Quantized to 8-bit for efficiency.",
      "runtime": "llamacpp",
      "sizeBytes": 650000000,
      "downloadUrl": "https://huggingface.co/Qwen/Qwen3-0.6B-FP8-GGUF/resolve/main/qwen3-0_6b-fp8.gguf",
      "filename": "qwen3-0.6b-q8.gguf",
      "config": {
        "contextLength": 2048,
        "temperature": 0.7,
        "gpuLayers": 32
      },
      "sha256": null
    },
    {
      "id": "tinyllama-1.1b-q4",
      "name": "TinyLLaMA 1.1B (Q4)",
      "description": "Lightweight and fast chat model, great for general conversation.",
      "runtime": "llamacpp",
      "sizeBytes": 400000000,
      "downloadUrl": "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
      "filename": "tinyllama-1.1b-q4.gguf",
      "config": {
        "contextLength": 2048,
        "temperature": 0.8,
        "gpuLayers": 28
      },
      "sha256": null
    },
    {
      "id": "phi3-mini-q4",
      "name": "Phi-3 Mini (Q4)",
      "description": "Microsoft's efficient 3.8B model, excellent for reasoning and code.",
      "runtime": "llamacpp",
      "sizeBytes": 2100000000,
      "downloadUrl": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf",
      "filename": "phi3-mini-q4.gguf",
      "config": {
        "contextLength": 4096,
        "temperature": 0.7,
        "gpuLayers": 32
      },
      "sha256": null
    },
    {
      "id": "gemma-2b-q4",
      "name": "Gemma 2B (Q4)",
      "description": "Google's compact model with strong performance across tasks.",
      "runtime": "llamacpp",
      "sizeBytes": 1500000000,
      "downloadUrl": "https://huggingface.co/google/gemma-2b-it-GGUF/resolve/main/gemma-2b-it-q4_k_m.gguf",
      "filename": "gemma-2b-q4.gguf",
      "config": {
        "contextLength": 8192,
        "temperature": 0.7,
        "gpuLayers": 26
      },
      "sha256": null
    }
  ]
}
